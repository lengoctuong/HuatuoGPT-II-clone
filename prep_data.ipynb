{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "token = os.getenv(\"HG_KEY\")\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructure huatuo data (AI Farm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65765it [00:06, 10316.82it/s]\n",
      "57080it [00:03, 16859.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_huatuo_data(\n",
    "    file_path: str,\n",
    "    dataset_type: str,  # 'train', 'valid', 'test'\n",
    "    output_data: Dict[str, List[List[str]]],\n",
    "    percent: float = 100,\n",
    "    shuffle: bool = False\n",
    "):\n",
    "    assert dataset_type in {'train', 'valid', 'test'}, \"dataset_type must be one of 'train', 'valid', 'test'\"\n",
    "    assert 0 < percent <= 100, \"percent must be between 0 and 100\"\n",
    "\n",
    "    # Mapping ID ph·∫ßn chu·ªói sang lo·∫°i d·ªØ li·ªáu\n",
    "    classification_map = {\n",
    "        'HuatuoGPT2_Pretrain_Meidcal_Encyclopedia_cn': 'Meidcal_Encyclopedia_cn',\n",
    "        'HuatuoGPT2_Pretrain_Meidcal_Encyclopedia_en': 'Meidcal_Encyclopedia_en',\n",
    "        'huatuo_encyclopedia_qa': 'huatuo_encyclopedia_qa',\n",
    "        'huatuo_knowledge_graph_qa': 'huatuo_knowledge_graph_qa',\n",
    "    }\n",
    "\n",
    "    dataset_map = {\n",
    "        'train_datasets': 'train',\n",
    "        'validation_datasets': 'valid',\n",
    "        'test_datasets': 'test',\n",
    "    }\n",
    "\n",
    "    # D·ªØ li·ªáu t·∫°m th·ªùi d√πng ƒë·ªÉ l·ªçc percent\n",
    "    temp_data = defaultdict(list)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line)\n",
    "            src_id = data.get(\"id\", \"\")\n",
    "            matched_class = None\n",
    "            matched_type = None\n",
    "\n",
    "            for key, val in classification_map.items():\n",
    "                if key in src_id:\n",
    "                    matched_class = val\n",
    "\n",
    "            for key, val in dataset_map.items():\n",
    "                if key in src_id:\n",
    "                    matched_type = val\n",
    "\n",
    "            if matched_class is None:\n",
    "                continue\n",
    "\n",
    "            # L·ªçc theo dataset_type\n",
    "            if matched_type:\n",
    "                if matched_type != dataset_type:\n",
    "                    continue\n",
    "            else:\n",
    "                if dataset_type == 'train':\n",
    "                    matched_type = 'train'\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            instruction = data.get(\"instruction\", \"\").strip()\n",
    "            output = data.get(\"output\", \"\").strip()\n",
    "            temp_data[matched_class].append([instruction, output])\n",
    "\n",
    "    # C·∫Øt ph·∫ßn trƒÉm v√† shuffle n·∫øu c·∫ßn, c·∫≠p nh·∫≠t v√†o output_data\n",
    "    for key, examples in temp_data.items():\n",
    "        if shuffle:\n",
    "            random.shuffle(examples)\n",
    "        keep_n = int(len(examples) * (percent / 100))\n",
    "        output_data[key].extend(examples[:keep_n])\n",
    "\n",
    "output_data = defaultdict(list)\n",
    "process_huatuo_data('/mnt/c/Users/HOME/Downloads/HuatuoGPT-II/all_data/huatuogpt2_vi.jsonl', 'valid', output_data, percent=5)\n",
    "process_huatuo_data('/mnt/c/Users/HOME/Downloads/HuatuoGPT-II/all_data/huatuo26m_vi.jsonl', 'valid', output_data, percent=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'huatuo_knowledge_graph_qa': 2, 'huatuo_encyclopedia_qa': 10}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = {}\n",
    "for k in output_data:\n",
    "    s[k] = len(output_data[k])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/c/Users/HOME/Downloads/HuatuoGPT-II/all_data/huatuo_validated_5p.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "def convert_jsonl_to_huatuo_format(file1, file2, output_file, percent=1):\n",
    "    # C√°c lo·∫°i id h·ª£p l·ªá\n",
    "    known_types = [\n",
    "        \"Meidcal_Web_Corpus_en\", \"Meidcal_Web_Corpus_cn\",\n",
    "        \"Meidcal_Literature_cn\", \"Meidcal_Literature_en\",\n",
    "        \"Meidcal_Encyclopedia_cn\", \"Meidcal_Encyclopedia_en\",\n",
    "        \"Meidcal_Books_cn\", \"Meidcal_Books_en\",\n",
    "        \"SFT_data\"\n",
    "    ]\n",
    "\n",
    "    def load_jsonl(filepath):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "    # Load d·ªØ li·ªáu\n",
    "    data1 = load_jsonl(file1)\n",
    "    data2 = load_jsonl(file2)\n",
    "\n",
    "    # C·∫Øt n%\n",
    "    n1 = int(len(data1) * percent)\n",
    "    n2 = int(len(data2) * percent)\n",
    "    data1 = data1[:n1]\n",
    "    data2 = data2[:n2]\n",
    "\n",
    "    # D·ªØ li·ªáu k·∫øt qu·∫£\n",
    "    merged_data = {}\n",
    "\n",
    "    # X·ª≠ l√Ω file1\n",
    "    for item in data1:\n",
    "        id_str = item.get(\"id\", \"\")\n",
    "        key = None\n",
    "        for k in known_types:\n",
    "            if k in id_str:\n",
    "                key = k\n",
    "                break\n",
    "        if not key:\n",
    "            key = \"huatuo_knowledge_graph_qa\"  # fallback (tr∆∞·ªùng h·ª£p sai)\n",
    "        q, a = item[\"instruction\"].strip(), item[\"output\"].strip()\n",
    "        if not q or not a:\n",
    "            continue\n",
    "        merged_data.setdefault(key, []).append([q, a])\n",
    "\n",
    "    # X·ª≠ l√Ω file2\n",
    "    for item in data2:\n",
    "        id_str = item.get(\"id\", \"\")\n",
    "        matched = any(k in id_str for k in known_types)\n",
    "        key = \"huatuo_knowledge_graph_qa\" if not matched else \"UNKNOWN_TYPE\"\n",
    "        if key == \"UNKNOWN_TYPE\":\n",
    "            print('UNKNOWN_TYPE', item)\n",
    "            continue  # skip b·∫•t th∆∞·ªùng\n",
    "\n",
    "        q, a = item[\"instruction\"].strip(), item[\"output\"].strip()\n",
    "        if not q or not a:\n",
    "            continue\n",
    "        merged_data.setdefault(key, []).append([q, a])\n",
    "\n",
    "    # Ghi k·∫øt qu·∫£\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Saved merged dataset to {output_file}\")\n",
    "    for key, items in merged_data.items():\n",
    "        print(f\" - {key}: {len(items)} samples\")\n",
    "\n",
    "# üîß V√≠ d·ª• c√°ch g·ªçi:\n",
    "p = 0.1/100\n",
    "file1 = \"/mnt/c/Users/HOME/Downloads/HuatuoGPT-II/all_data/huatuogpt2_vi.jsonl\"\n",
    "file2 = \"/mnt/c/Users/HOME/Downloads/HuatuoGPT-II/all_data/huatuo26m_vi.jsonl\"\n",
    "convert_jsonl_to_huatuo_format(\n",
    "    file1=file1,\n",
    "    file2=file2,\n",
    "    output_file=os.path.join(os.path.dirname(file1), \"train_qa_\" + str(p*100) + \"p_vi.json\"),\n",
    "    percent=p\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check unique ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11_HuatuoGPT2_chinese/HuatuoGPT2_Pretrain_Meidcal_Encyclopedia_cn.json: 43888\n",
      "11_HuatuoGPT2_chinese/HuatuoGPT2_Pretrain_Meidcal_Encyclopedia_en.json: 21877\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Dictionary to store counts of unique base IDs\n",
    "id_counts = {}\n",
    "\n",
    "with open('/mnt/c/Users/HOME/Downloads/HuatuoGPT-II/all_data/huatuogpt2_vi.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        full_id = data['id']\n",
    "        # Split to get the base ID (before ---)\n",
    "        base_id = full_id.split('---')[0]\n",
    "        # Increment count for this base_id\n",
    "        id_counts[base_id] = id_counts.get(base_id, 0) + 1\n",
    "\n",
    "# Print results with counts\n",
    "for uid in sorted(id_counts.keys()):\n",
    "    print(f\"{uid}: {id_counts[uid]}\")\n",
    "\n",
    "# Optionally save to file\n",
    "with open('huotuogpt2_unique_ids.txt', 'w', encoding='utf-8') as f:\n",
    "    for uid in sorted(id_counts.keys()):\n",
    "        f.write(f\"{uid}: {id_counts[uid]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check null input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "input_file = \"/mnt/c/Users/HOME/Downloads/HuatuoGPT-II/all_data/huatuo26m_vi.jsonl\"  # Thay b·∫±ng ƒë∆∞·ªùng d·∫´n file th·∫≠t\n",
    "output_data = []\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        instruction = item.get(\"instruction\", \"\").strip()\n",
    "        input_field = item.get(\"input\", \"\").strip()\n",
    "        output = item.get(\"output\", \"\").strip()\n",
    "\n",
    "        # In ra n·∫øu input kh√°c r·ªóng\n",
    "        if input_field:\n",
    "            print(f\"[Non-empty input] ID: {item.get('id')}, input: {input_field}\")\n",
    "\n",
    "        # G·ªôp input v√†o instruction n·∫øu c√≥\n",
    "        if input_field:\n",
    "            full_instruction = f\"{instruction}\\n\\n{input_field}\"\n",
    "        else:\n",
    "            full_instruction = instruction\n",
    "\n",
    "        # Th√™m v√†o d·∫°ng multi-turn [\"q\", \"a\"]\n",
    "        output_data.append([full_instruction, output])\n",
    "\n",
    "# Chuy·ªÉn th√†nh ƒë·ªãnh d·∫°ng m√† HuatuoGPT_data c√≥ th·ªÉ ƒë·ªçc:\n",
    "formatted_data = [qa_pair for qa_pair in output_data]  # m·ªói ph·∫ßn t·ª≠ l√† [\"question\", \"answer\"]\n",
    "\n",
    "# L∆∞u ra file n·∫øu mu·ªën:\n",
    "with open(os.path.join(os.path.dirname(input_file), \"train_\" + os.path.basename(input_file)[:-1]), \"w\", encoding=\"utf-8\") as out_f:\n",
    "    json.dump({\"SFT_data\": output_data}, out_f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ ƒê√£ x·ª≠ l√Ω {len(output_data)} m·∫´u. L∆∞u v√†o 'formatted_sft_data.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructure huatuo data (Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ijson\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "# List of data sources\n",
    "data_sources = [\n",
    "    \"Meidcal_Web_Corpus_en\",\n",
    "    \"Meidcal_Web_Corpus_cn\",\n",
    "    \"Meidcal_Literature_cn\",\n",
    "    \"Meidcal_Literature_en\",\n",
    "    \"Meidcal_Encyclopedia_cn\",\n",
    "    \"Meidcal_Encyclopedia_en\",\n",
    "    \"Meidcal_Books_cn\",\n",
    "    \"Meidcal_Books_en\",\n",
    "    \"SFT_data\"\n",
    "]\n",
    "\n",
    "# Initialize the final dictionary\n",
    "final_dict = {}\n",
    "\n",
    "# Function to count total conversations in a JSON file\n",
    "def count_conversations(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            parser = ijson.items(f, 'item')\n",
    "            return sum(1 for _ in parser)  # Count all conversations\n",
    "    except Exception as e:\n",
    "        print(f\"Error counting conversations in {file_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to process a chunk of conversations (first 1%)\n",
    "def process_json_chunk(file_path, num_to_include):\n",
    "    conversations = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            parser = ijson.items(f, 'item')\n",
    "            for i, conv in enumerate(parser):\n",
    "                if i >= num_to_include:  # Stop after 1%\n",
    "                    break\n",
    "                # Extract the \"value\" fields from conversations\n",
    "                turns = [turn[\"value\"] for turn in conv[\"conversations\"]]\n",
    "                conversations.append(turns)\n",
    "        return conversations\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Process each data source\n",
    "used_percent = .01/100\n",
    "\n",
    "for source in data_sources:\n",
    "    if source == \"SFT_data\":\n",
    "        file_path = os.path.join(\"/mnt/c/Users/HOME/Downloads\", f\"HuatuoGPT2-GPT4-SFT-140K.json\")  # Adjust path as needed\n",
    "    else:\n",
    "        file_path = os.path.join(\"/mnt/c/Users/HOME/Downloads\", f\"HuatuoGPT2_Pretrain_{source}.json\")  # Adjust path as needed\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Count total conversations to determine 1%\n",
    "        total_conversations = count_conversations(file_path)\n",
    "        num_to_include = math.ceil(total_conversations * used_percent) if total_conversations > 0 else 0\n",
    "        # Process only the first 1% of conversations\n",
    "        if num_to_include > 0:\n",
    "            final_dict[source] = process_json_chunk(file_path, num_to_include)\n",
    "        else:\n",
    "            final_dict[source] = []\n",
    "            print(f\"No conversations found in {file_path}\")\n",
    "\n",
    "# Save the combined JSON\n",
    "output_path = '/mnt/c/Users/HOME/Downloads/HuatuoGPT-II/adaption/one_stage_training/train_qa_' + str(used_percent*100) + 'p.json'\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Processing complete. Output saved to combined.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cn to En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from argostranslate import package, translate\n",
    "\n",
    "# === 1. C√†i ƒë·∫∑t g√≥i d·ªãch n·∫øu ch∆∞a c√≥ ===\n",
    "package.update_package_index()\n",
    "available_packages = package.get_available_packages()\n",
    "zh_vi_package = next(p for p in available_packages if p.from_code == \"zh\" and p.to_code == \"en\")\n",
    "package.install_from_path(zh_vi_package.download())\n",
    "\n",
    "# === 2. ƒê·ªçc file JSON g·ªëc ===\n",
    "input_path = '/mnt/c/Users/HOME/Downloads/HuatuoGPT-II/adaption/one_stage_training/train_qa_0.01p.json'\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print('loaded')\n",
    "\n",
    "# === 3. L·∫∑p v√† d·ªãch t·ª´ng c√¢u h·ªèi + ƒë√°p ===\n",
    "translated_data = {}\n",
    "\n",
    "for key, qa_list in data.items():\n",
    "    translated_pairs = []\n",
    "    for question, answer in qa_list:\n",
    "        translated_q = translate.translate(question, \"zh\", \"en\")\n",
    "        translated_a = translate.translate(answer, \"zh\", \"en\")\n",
    "        translated_pairs.append([translated_q, translated_a])\n",
    "    # ƒê·∫∑t t√™n m·ªõi cho key\n",
    "    translated_key = key.replace(\"_en\", \"_vi\")\n",
    "    translated_data[translated_key] = translated_pairs\n",
    "    print(key)\n",
    "\n",
    "# === 4. Ghi ra file JSON m·ªõi ===\n",
    "with open(input_path[:-5] + '_vi.json', \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ d·ªãch v√† l∆∞u v√†o output_vi.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huatuo",
   "language": "python",
   "name": "lnt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
