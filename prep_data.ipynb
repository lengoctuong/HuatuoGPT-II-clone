{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "token = os.getenv(\"HG_KEY\")\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Output saved to combined.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ijson\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "# List of data sources\n",
    "data_sources = [\n",
    "    \"Meidcal_Web_Corpus_en\",\n",
    "    \"Meidcal_Web_Corpus_cn\",\n",
    "    \"Meidcal_Literature_cn\",\n",
    "    \"Meidcal_Literature_en\",\n",
    "    \"Meidcal_Encyclopedia_cn\",\n",
    "    \"Meidcal_Encyclopedia_en\",\n",
    "    \"Meidcal_Books_cn\",\n",
    "    \"Meidcal_Books_en\",\n",
    "    \"SFT_data\"\n",
    "]\n",
    "\n",
    "# Initialize the final dictionary\n",
    "final_dict = {}\n",
    "\n",
    "# Function to count total conversations in a JSON file\n",
    "def count_conversations(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            parser = ijson.items(f, 'item')\n",
    "            return sum(1 for _ in parser)  # Count all conversations\n",
    "    except Exception as e:\n",
    "        print(f\"Error counting conversations in {file_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to process a chunk of conversations (first 1%)\n",
    "def process_json_chunk(file_path, num_to_include):\n",
    "    conversations = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            parser = ijson.items(f, 'item')\n",
    "            for i, conv in enumerate(parser):\n",
    "                if i >= num_to_include:  # Stop after 1%\n",
    "                    break\n",
    "                # Extract the \"value\" fields from conversations\n",
    "                turns = [turn[\"value\"] for turn in conv[\"conversations\"]]\n",
    "                conversations.append(turns)\n",
    "        return conversations\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Process each data source\n",
    "used_percent = .01/100\n",
    "\n",
    "for source in data_sources:\n",
    "    if source == \"SFT_data\":\n",
    "        file_path = os.path.join(\"/mnt/c/Users/HOME/Downloads\", f\"HuatuoGPT2-GPT4-SFT-140K.json\")  # Adjust path as needed\n",
    "    else:\n",
    "        file_path = os.path.join(\"/mnt/c/Users/HOME/Downloads\", f\"HuatuoGPT2_Pretrain_{source}.json\")  # Adjust path as needed\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Count total conversations to determine 1%\n",
    "        total_conversations = count_conversations(file_path)\n",
    "        num_to_include = math.ceil(total_conversations * used_percent) if total_conversations > 0 else 0\n",
    "        # Process only the first 1% of conversations\n",
    "        if num_to_include > 0:\n",
    "            final_dict[source] = process_json_chunk(file_path, num_to_include)\n",
    "        else:\n",
    "            final_dict[source] = []\n",
    "            print(f\"No conversations found in {file_path}\")\n",
    "\n",
    "# Save the combined JSON\n",
    "output_path = '/mnt/c/Users/HOME/Downloads/HuatuoGPT-II/adaption/one_stage_training/train_qa_' + str(used_percent*100) + 'p.json'\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Processing complete. Output saved to combined.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meidcal_Web_Corpus_en\n",
      "Meidcal_Web_Corpus_cn\n",
      "Meidcal_Literature_cn\n",
      "Meidcal_Literature_en\n",
      "Meidcal_Encyclopedia_cn\n",
      "Meidcal_Encyclopedia_en\n",
      "Meidcal_Books_cn\n",
      "Meidcal_Books_en\n",
      "SFT_data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(key)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# === 4. Ghi ra file JSON mới ===\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path[:-\u001b[32m4\u001b[39m] + \u001b[33m'\u001b[39m\u001b[33m_vi.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     31\u001b[39m     json.dump(translated_data, f, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m2\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Đã dịch và lưu vào output_vi.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'output_path' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from argostranslate import package, translate\n",
    "\n",
    "# === 1. Cài đặt gói dịch nếu chưa có ===\n",
    "package.update_package_index()\n",
    "available_packages = package.get_available_packages()\n",
    "zh_vi_package = next(p for p in available_packages if p.from_code == \"zh\" and p.to_code == \"en\")\n",
    "package.install_from_path(zh_vi_package.download())\n",
    "\n",
    "# === 2. Đọc file JSON gốc ===\n",
    "input_path = '/mnt/c/Users/HOME/Downloads/HuatuoGPT-II/adaption/one_stage_training/train_qa_0.01p.json'\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print('loaded')\n",
    "\n",
    "# === 3. Lặp và dịch từng câu hỏi + đáp ===\n",
    "translated_data = {}\n",
    "\n",
    "for key, qa_list in data.items():\n",
    "    translated_pairs = []\n",
    "    for question, answer in qa_list:\n",
    "        translated_q = translate.translate(question, \"zh\", \"en\")\n",
    "        translated_a = translate.translate(answer, \"zh\", \"en\")\n",
    "        translated_pairs.append([translated_q, translated_a])\n",
    "    # Đặt tên mới cho key\n",
    "    translated_key = key.replace(\"_en\", \"_vi\")\n",
    "    translated_data[translated_key] = translated_pairs\n",
    "    print(key)\n",
    "\n",
    "# === 4. Ghi ra file JSON mới ===\n",
    "with open(input_path[:-5] + '_vi.json', \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Đã dịch và lưu vào output_vi.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /mnt/c/Users/HOME/Downloads/HuatuoGPT-II; python adaption/one_stage_training/data_process.py --model_path \"baichuan-inc/Baichuan2-13B-Base\" --data_path \"adaption/one_stage_training/sample_train_qa.json\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huatuo",
   "language": "python",
   "name": "lnt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
